
# 📊 나이브 베이즈 확률

## 🤖 머신러닝의 3가지 종류

1. **📘 지도 학습:** 정답이 있는 데이터로 기계가 학습
   - 예시: "프랑스의 수도?" → "파리"
   - 알고리즘: KNN, 나이브 베이즈, 회귀
   - 데이터 예시: 유방암 데이터, 아이리스 데이터, 와인 데이터, 유리 데이터

2. **📙 비지도 학습:** 정답 없이 데이터의 패턴과 구조를 학습하는 학습 방법
   - 예시: "the quick brown fox jumps over the" → "lazy dog 가능성이 높다."

3. **📗 강화 학습:** 주어진 환경을 기계가 스스로 이해하면서 데이터를 만들어가며 학습

---

## 🏆 나이브 베이즈 알고리즘의 활용 분야

1. **📩 스팸 이메일 필터링**
2. **🛡 네트워크 침입 탐지**
3. **🏥 의학적 질병 진단**

### 💡 실전 사례: 랜섬웨어 탐지 시스템
**기업:** [Sands Lab](https://www.sandslab.io/)

- 컴퓨터 내 문서에서 악성 코드가 포함된 파일 탐지하는 기술

---

## 📌 나이브 베이즈 확률 공식

### 🔢 주요 개념
1. **사전확률**: 이미 알고 있는 사건의 확률
2. **우도**: 특정 조건이 주어졌을 때 다른 사건이 발생할 확률
3. **사후확률**: 사전확률과 우도를 이용해 계산하는 조건부 확률

### 📧 예제: "비아그라" 단어가 포함된 메일이 스팸일 확률 계산

P(스팸 | 비아그라) = (P(비아그라 | 스팸) * P(스팸)) / P(비아그라)

#### 📊 빈도표 → 우도표 변환
| 유형  | 비아그라 있음 | 비아그라 없음 | 총계 |
|------|------------|------------|----|
| 스팸 | 4         | 16         | 20 |
| 햄   | 1         | 79         | 80 |
| 총계 | 5         | 95         | 100 |

#### 🧮 확률 계산
P(스팸 | 비아그라) = (4/20 * 20/100) / (5/100) = 0.8

0.8 (80%)이므로 해당 이메일은 스팸으로 분류됨.

---

## 🔢 다중 단어가 포함된 경우

P(스팸 | 비아그라 ∩ 돈 ∩ 식료품 ∩ 구독취소) = ?

- 단일 단어만으로는 정확한 분류가 어려우므로, 여러 단어를 고려하여 확률을 계산해야 함.
- 수식 적용:

P(스팸 | 비아그라 ∩ ￢ 돈 ∩ ￢ 식료품 ∩ 구독취소) = (P(비아그라 | 스팸) * P(￢ 돈 | 스팸) * P(￢ 식료품 | 스팸) * P(구독취소 | 스팸) * P(스팸)) / P(비아그라 ∩ ￢ 돈 ∩ ￢ 식료품 ∩ 구독취소)

### 🤔 분모 무시 가능 이유

- 상대적인 확률값만 비교하면 되므로, 분모를 제외하고 분자 값만 비교 가능.

#### 🎯 계산 결과
| 유형  | 전체 우도 |
|------|---------|
| 스팸 | 0.012   |
| 햄   | 0.002   |

P(스팸) = 0.012 / (0.012 + 0.002) = 0.857 (85.7%)

P(햄) = 0.002 / (0.012 + 0.002) = 0.143 (14.3%)

해당 메일은 **85.7% 확률로 스팸**이고, **14.3% 확률로 정상 메일**로 분류됨.

---

## 📌 확률 기반 기술 응용

- **📧 이메일 스팸 필터링**
- **🧬 의료 진단 예측**
- **💰 금융 사기 탐지**
- **🎬 영화 추천 시스템**

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 🎥 나이브 베이즈 확률 실습

## 📂 데이터: movie2.csv

🔗 데이터 다운로드: [movie2.csv](https://cafe.daum.net/oracleoracle/Soei/56)

---

## 📌 1. 데이터 불러오기
```{r}
library(readr)
movie <- read.csv("c:\\data\\movie2.csv", stringsAsFactors=TRUE)
View(movie)
dim(movie)  # 39행 6열
```

---

## 🔍 2. 결측치 확인
```{r}
colSums(is.na(movie))
```

---

## 📊 3. 데이터 탐색
```{r}
str(movie)
```

---

## ✂ 4. 훈련 데이터와 테스트 데이터로 분리
```{r}
library(caret)
set.seed(1)
k <- createDataPartition(movie$장르, p=0.8, list=FALSE)
train_data <- movie[k, ]
test_data <- movie[-k, ]
nrow(train_data)  # 훈련 데이터 개수
nrow(test_data)   # 테스트 데이터 개수
```

---

## 🏗 5. 나이브 베이즈 모델 생성
```{r}
library(e1071)
model <- naiveBayes(장르 ~ ., data=train_data)
model
```

**📌 해석:**
1. **사전확률:** 각 장르의 출현 확률
2. **조건부 확률:** 나이, 성별, 직업, 결혼 여부, 이성친구 여부에 따른 확률

---

## 🎯 6. 모델로 테스트 데이터 예측
```{r}
result <- predict(model, test_data[, -6])  # 정답(장르) 컬럼 제외
result
```

---

## 📈 7. 모델 성능 평가
```{r}
accuracy <- sum(result == test_data[, 6]) / length(test_data[, 6]) * 100
accuracy  # 정확도 출력
```

---

## 📊 8. 이원 교차표 (Confusion Matrix)
```{r}
library(gmodels)
a <- CrossTable(test_data[, 6], result)
a$t  # 이원 교차표 출력
```

---

## 🎬 9. 맞춤형 예측 수행

🎭 **예측 예제 1:**
- 20대 남성, 학생, 결혼 안 함, 이성친구 없음 → 선호 영화 장르 예측

```{r}
test_data2 <- data.frame(나이='20대', 성별='남', 직업='학생', 결혼여부='NO', 이성친구='NO')
result <- predict(model, test_data2)
result  # 예측 결과
```

---

## 🎯 10. 확률 기반 예측 수행

🔄 **확률까지 출력하는 모델 생성**
```{r}
library(naivebayes)
new_model <- naive_bayes(장르 ~ ., data=train_data)
```

🎭 **예측 예제 2:**
- 20대 남성, 학생, 결혼 안 함, 이성친구 없음 → 선호 영화 장르 확률 예측
```{r}
result2 <- predict(new_model, test_data2, type='prob')
result2  # 확률 출력
```

---

## ❓ 연습문제

### 📝 문제 1
**40대 남성, 언론 직업, 결혼, 이성친구 없음 → 공포 영화 선호 확률?**
```{r}
test_data3 <- data.frame(나이='40대', 성별='남', 직업='언론', 결혼여부='YES', 이성친구='NO')
result3 <- predict(new_model, test_data3, type='prob')
result3  # 공포 영화 확률 출력
```

### 📝 문제 2
**20대 여성, 학생, 결혼 안 함, 이성친구 있음 → 선호 영화 장르 예측?**
```{r}
test_data4 <- data.frame(나이='20대', 성별='여', 직업='학생', 결혼여부='NO', 이성친구='YES')
result4 <- predict(new_model, test_data4, type='prob')
result4  # 확률 출력
```


